{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56264cf-7bb5-427f-861f-a3a127440d42",
   "metadata": {},
   "source": [
    "# Training process\n",
    "\n",
    "The data set generated in `generate_data.py` can now be used to train a normalising flow neural network to perform gravity inversion. For this part of the work it is recommended to use a GPU as this will provide a significant speed-up in computation times.\n",
    "\n",
    "To train the network, first we need to define some directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cfe22e5-7f59-4bf6-bb8f-8badf92028c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data = 'data' # where our training and validation that are located\n",
    "save = 'trained_flow' # where we want to save our outputs\n",
    "\n",
    "if not os.path.exists(save):\n",
    "    os.mkdir(save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a615d4-fbfe-4598-93f4-7d58e39c4887",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Now we need to read in our generated data set and reformat it. In this stage we also need to define what information to show the network during training. This can be set in the inputs to the `BoxDataset.make_data_for_network()` function. We need to carefully think about what information we want to and we have to include for the data to be interpretable for the normalising flow, and so that we only marginalise over parameters we are not interested in. \n",
    "\n",
    "For example, in this case we define our survey points in constant locations, therefore we do not need to provide the coordinates of these points. Since we don't provide these coordinates, mixing the order of the survey points would confuse the network, therefore we set `mix_survey_order = False`. Finally, we allowed a range of possible noise realisations when creating our data set. We want to be able to tell the network of the scale of the noise on the specific gravimetry survey we are inverting, therefore we include the `'noise_scale'` in our conditional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee97c0b6-201b-4181-b85d-b55b45921a87",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/trainset.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgiflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BoxDataset\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainset.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m     dt \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      5\u001b[0m     train_data, train_conditional \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mmake_data_for_network(\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# Since mix_survey_order=True we need the information below for the network to identify the survey locations\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         survey_coordinates_to_include \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         mix_survey_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/exenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/trainset.pkl'"
     ]
    }
   ],
   "source": [
    "from giflow.box import BoxDataset\n",
    "\n",
    "with open(os.path.join(data, 'trainset.pkl'), 'rb') as file:\n",
    "    dt = pkl.load(file)\n",
    "    train_data, train_conditional = dt.make_data_for_network(\n",
    "        survey_coordinates_to_include = ['noise_scale'],\n",
    "        model_info_to_include = [],\n",
    "        add_noise = True, # This refers to survey noise.\n",
    "        mix_survey_order = False\n",
    "    )\n",
    "\n",
    "# Need to do this with the validation data too\n",
    "with open(os.path.join(data, 'validationset.pkl'), 'rb') as file:\n",
    "    dt = pkl.load(file)\n",
    "    validation_data, validation_conditional = dt.make_data_for_network(\n",
    "        survey_coordinates_to_include = ['noise_scale'],\n",
    "        model_info_to_include = [],\n",
    "        add_noise = True,\n",
    "        mix_survey_order = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b95ffad-006c-4394-ae50-77ee45b3085f",
   "metadata": {},
   "source": [
    "The output from this function is a list of arrays all containing different information, with the first array in the `train_data` always containing the source model parameters, and the first array in `train_conditional` containing the gravity values. This can be directly sent to the `Scaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edba9d-f812-4a59-b6dd-9507ef64d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from giflow.scaler import Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scaling the data first\n",
    "sc_data = Scaler(scalers = [MinMaxScaler()]) # Need to define the scaler for each element in the train_data list.\n",
    "sc_data.scale_data(train_data, fit = True) # Fit the scaler and store in the class\n",
    "\n",
    "sc_conditional = Scaler(scalers = [MinMaxScaler(), MinMaxScaler()]\n",
    "sc_conditional.scale_data(train_conditional, fit = True)\n",
    "\n",
    "scalers = {'conditional': sc_conditional, 'data': sc_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657a274-0727-4e68-8708-e531031d477b",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Now it is time to define the normalising flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50d4b6-0f75-41c5-97ae-3abe768e1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from giflow.flowmodel import FlowModel\n",
    "\n",
    "hyperparameters = {\n",
    "        'n_inputs': 7, # the total number of parameters in the source model, including any additional information we chose to include\n",
    "        'n_conditional_inputs': 65, # the total number of values in the conditional\n",
    "        'n_transforms': 12,\n",
    "        'n_blocks_per_transform': 2,\n",
    "        'n_neurons': 64,\n",
    "        # The parameters below define some settings for the training\n",
    "        'batch_size': 5000,\n",
    "        'batch_norm': True,\n",
    "        'lr': 0.001,\n",
    "        'epochs': 3000,\n",
    "        'early_stopping': False # if set True, the training stops when the validation loss stops decreasing\n",
    "}\n",
    "\n",
    "# Construct the flow\n",
    "flow.save_location = save\n",
    "flow.data_location = data\n",
    "flow = FlowModel(\n",
    "        hyperparameters = hyperparameters,\n",
    "        datasize = datasize,\n",
    "        scalers = scalers\n",
    ")\n",
    "save_flow(flow)\n",
    "flow.construct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250714fa-d604-4e6b-8764-fefccb3aa839",
   "metadata": {},
   "source": [
    "It is required to define an optimiser for the training and we can choose to use a scheduler as well to make the training more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8bbcd-59b9-4958-bdbe-43470ca494aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(\n",
    "    flow.flowmodel.parameters(),\n",
    "    lr = flow.hyperparameters['lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46682ec4-0f57-40db-b2bb-fbbf8d6f4cf9",
   "metadata": {},
   "source": [
    "# An example scheduler:\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimiser,\n",
    "    mode = 'min',\n",
    "    factor = 0.05,\n",
    "    patience = 90,\n",
    "    cooldown = 10,\n",
    "    min_lr = 1e-6,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b97a15-d864-4ef5-ad66-13ce7d29288f",
   "metadata": {},
   "source": [
    "Now that everything is set up, we can send our data to the desired device and start training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78d40d-2c9f-4d91-8b97-297922bbda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') # Define the GPU we want to train on\n",
    "\n",
    "# Make the tensor data sets\n",
    "train_dataset = flow.make_tensor_dataset(\n",
    "    train_data, \n",
    "    train_conditional, \n",
    "    device = device, \n",
    "    scale = True\n",
    ")\n",
    "\n",
    "validation_dataset = flow.make_tensor_dataset(\n",
    "    validation_data, \n",
    "    validation_conditional, \n",
    "    device = device, \n",
    "    scale = True\n",
    ")\n",
    "\n",
    "flow.train(\n",
    "    optimiser = optimiser,\n",
    "    validation_dataset = validation_dataset,\n",
    "    train_dataset = train_dataset,\n",
    "    scheduler = scheduler,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465e990-4592-4a5a-9bb5-cf3c185c8d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
