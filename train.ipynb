{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56264cf-7bb5-427f-861f-a3a127440d42",
   "metadata": {},
   "source": [
    "# Training process\n",
    "\n",
    "The data set generated in `generate_data.py` can now be used to train a normalising flow neural network to perform gravity inversion. For this part of the work it is recommended to use a GPU as this will provide a significant speed-up in computation times.\n",
    "\n",
    "To train the network, first we need to define some directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfe22e5-7f59-4bf6-bb8f-8badf92028c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data = 'data' # where our training and validation that are located\n",
    "save = 'trained_flow' # where we want to save our outputs\n",
    "\n",
    "if not os.path.exists(save):\n",
    "    os.mkdir(save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a615d4-fbfe-4598-93f4-7d58e39c4887",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Now we need to read in our generated data set and reformat it. In this stage we also need to define what information to show the network during training. This can be set in the inputs to the `BoxDataset.make_data_for_network()` function. We need to carefully think about what information we want to and we have to include for the data to be interpretable for the normalising flow, and so that we only marginalise over parameters we are not interested in. \n",
    "\n",
    "For example, in this case we define our survey points in constant locations, therefore we do not need to provide the coordinates of these points. Since we don't provide these coordinates, mixing the order of the survey points would confuse the network, therefore we set `mix_survey_order = False`. Finally, we allowed a range of possible noise realisations when creating our data set. We want to be able to tell the network of the scale of the noise on the specific gravimetry survey we are inverting, therefore we include the `'noise_scale'` in our conditional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee97c0b6-201b-4181-b85d-b55b45921a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from giflow.box import BoxDataset\n",
    "import pickle as pkl\n",
    "\n",
    "# Reading in files\n",
    "trainsize = 5000\n",
    "with open(os.path.join(data, 'trainset.pkl'), 'rb') as file:\n",
    "    dt = pkl.load(file)\n",
    "    train_data, train_conditional = dt.make_data_for_network(\n",
    "        survey_coordinates_to_include = ['noise_scale'],\n",
    "        model_info_to_include = [],\n",
    "        add_noise = True, # This refers to survey noise.\n",
    "        mix_survey_order = False\n",
    "    )\n",
    "\n",
    "# Need to do this with the validation data too\n",
    "valsize = 500\n",
    "with open(os.path.join(data, 'valset.pkl'), 'rb') as file:\n",
    "    dt = pkl.load(file)\n",
    "    validation_data, validation_conditional = dt.make_data_for_network(\n",
    "        survey_coordinates_to_include = ['noise_scale'],\n",
    "        model_info_to_include = [],\n",
    "        add_noise = True,\n",
    "        mix_survey_order = False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b95ffad-006c-4394-ae50-77ee45b3085f",
   "metadata": {},
   "source": [
    "The output from this function is a list of arrays all containing different information, with the first array in the `train_data` always containing the source model parameters, and the first array in `train_conditional` containing the gravity values. This can be directly sent to the `Scaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25edba9d-f812-4a59-b6dd-9507ef64d087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting scaler and compressor to data set...\n",
      "Fitting scaler and compressor to data set...\n"
     ]
    }
   ],
   "source": [
    "from giflow.scaler import Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc_data = Scaler(scalers = [MinMaxScaler()]) # Need to define the scaler for each element in the train_data list.\n",
    "sc_data.scale_data(train_data, fit = True) # Fit the scaler and store in the class\n",
    "\n",
    "sc_conditional = Scaler(scalers = [MinMaxScaler(), MinMaxScaler()])\n",
    "sc_conditional.scale_data(train_conditional, fit = True)\n",
    "\n",
    "scalers = {'conditional': sc_conditional, 'data': sc_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657a274-0727-4e68-8708-e531031d477b",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Now it is time to define the normalising flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc50d4b6-0f75-41c5-97ae-3abe768e1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from giflow.flowmodel import FlowModel, save_flow\n",
    "\n",
    "hyperparameters = {\n",
    "        'n_inputs': 7, # the total number of parameters in the source model, including any additional information we chose to include\n",
    "        'n_conditional_inputs': 65, # the total number of values in the conditional\n",
    "        'n_transforms': 12,\n",
    "        'n_blocks_per_transform': 2,\n",
    "        'n_neurons': 64,\n",
    "        # The parameters below define some settings for the training\n",
    "        'batch_size': 5000,\n",
    "        'batch_norm': True,\n",
    "        'lr': 0.001,\n",
    "        'epochs': 3000,\n",
    "        'early_stopping': False # if set True, the training stops when the validation loss stops decreasing\n",
    "}\n",
    "\n",
    "# Construct the flow\n",
    "flow = FlowModel(\n",
    "        hyperparameters = hyperparameters,\n",
    "        datasize = trainsize,\n",
    "        scalers = scalers\n",
    ");\n",
    "flow.save_location = save\n",
    "flow.data_location = data\n",
    "save_flow(flow)\n",
    "flow.construct();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250714fa-d604-4e6b-8764-fefccb3aa839",
   "metadata": {},
   "source": [
    "It is required to define an optimiser for the training and we can choose to use a scheduler as well to make the training more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e8bbcd-59b9-4958-bdbe-43470ca494aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(\n",
    "    flow.flowmodel.parameters(),\n",
    "    lr = flow.hyperparameters['lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46682ec4-0f57-40db-b2bb-fbbf8d6f4cf9",
   "metadata": {},
   "source": [
    "# An example scheduler:\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimiser,\n",
    "    mode = 'min',\n",
    "    factor = 0.05,\n",
    "    patience = 90,\n",
    "    cooldown = 10,\n",
    "    min_lr = 1e-6,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b97a15-d864-4ef5-ad66-13ce7d29288f",
   "metadata": {},
   "source": [
    "Now that everything is set up, we can send our data to the desired device and start training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a78d40d-2c9f-4d91-8b97-297922bbda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') # Define the GPU we want to train on\n",
    "\n",
    "# Make the tensor data sets\n",
    "train_dataset = flow.make_tensor_dataset(\n",
    "    train_data, \n",
    "    train_conditional, \n",
    "    device = device, \n",
    "    scale = True\n",
    ")\n",
    "\n",
    "validation_dataset = flow.make_tensor_dataset(\n",
    "    validation_data, \n",
    "    validation_conditional, \n",
    "    device = device, \n",
    "    scale = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465e990-4592-4a5a-9bb5-cf3c185c8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.train(\n",
    "    optimiser = optimiser,\n",
    "    validation_dataset = validation_dataset,\n",
    "    train_dataset = train_dataset,\n",
    "    scheduler = None,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aafad3-65a1-44ba-b1b3-c4b9fa4bdbd1",
   "metadata": {},
   "source": [
    "All the outputs, including diagnostics and loss plots are saved in the `trained_flow` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd9d2c-034e-4a3b-a0e1-cefc1e17d536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
